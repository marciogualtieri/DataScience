---
title: 'Fake News Proliferation: An Interactive Graphical Exploration'
author: "Marcio Gualtieri"
date: "13 February 2017"
output:
  html_notebook:
    css: ../styles/style.css
    toc: yes
    toc_depth: 4
  html_document:
    css: ../styles/style.css
    toc: yes
    toc_depth: 4
---

# Overview

The data used in this notebook has been taken from a survey performed by [BuzzFeed](https://www.buzzfeed.com/), which attempted to measure how effectual is the proliferation of fake news in different states in the United States. The original data can be found [here](https://github.com/BuzzFeedNews/2016-12-fake-news-survey) and the article that makes use of the data [here](https://www.buzzfeed.com/craigsilverman/fake-news-survey). 

After reading the article, I noticed that they could have used some interactivity in the graphs. They needed a somehow large number of static graphs to express their results. They could have been equally expressive with a much smaller number of interactive graphs, thus my choice of using this data-set to showcase my [leaflet](http://leafletjs.com/) skills.

This data-set required quite a bit of work, so if you are only interested in  interactive graphs, you can skip to the [Graphical Exploration](#graphicalexploration) section. 


# Installing the Required Packages

You might need to install the following packages if you don't already have them:

```{r, eval = FALSE}
install.packages("xtable")
install.packages("dplyr")
install.packages("plyr")
install.packages("stringr")
install.packages("leaflet")
install.packages("rgdal")
install.packages("raster")
install.packages("RColorBrewer")
install.packages("shiny")
install.packages("reshape2")
```

# Importing the Required Packages

Once the libraries are installed, they need to be loaded as follows:

```{r}
suppressMessages(library(sp))
suppressMessages(library(xtable))                        # Pretty printing dataframes
suppressMessages(library(plyr, warn.conflicts = FALSE))  # Manipulating dataframes
suppressMessages(library(dplyr, warn.conflicts = FALSE))
suppressMessages(library(stringr))                       # Manipulating strings
suppressMessages(library(leaflet))                       # Interactive graphs
suppressMessages(library(rgdal))
suppressMessages(library(raster))                        # Loading geo-spacial data
suppressMessages(library(RColorBrewer))                  # Preview palletes
suppressMessages(library(shiny))
suppressMessages(library(reshape2))                      # Reshape columns into rows
```

# Loading Data

## Reading CSV Files

Let's first load the data-sets:

```{r}
data <- read.csv("../input/raw-data.csv", na.strings=c("NA", "NULL", ""), stringsAsFactors = FALSE)
headlines <- read.csv("../input/headlines.csv", na.strings=c("NA", "NULL", ""), stringsAsFactors = FALSE)
```

## Taking a Sample

```{r, results="asis"}
render_table_in_viewer_pane <- function(data, digits) {
  html <- print(xtable(data, digits = digits), type = "html", print.results=FALSE)
  temp <- tempfile(fileext = ".html")
  cat(html, file = temp)
  rstudioapi::viewer(temp)
}

render_table <- function(data, digits = 2) {
  render_table_in_viewer_pane(data, digits)
  print(xtable(data, digits = digits), type = "html")
}

sample_data_frame <- function(data, size) {
  sample_index <- sample(1:nrow(data), size)
  return(data[sample_index, ])
}

render_table(sample_data_frame(data, 6))
```

## Record Counting

```{r, results="asis"}
data_count <- function(data) {
  output <-               data.frame(measurement = "records", count = nrow(data))
  output <- rbind(output, data.frame(measurement = "variables", count = length(names(data))))
  return(output)
}

render_table(data_count(data))
```

# Missing Data

```{r, results="asis"}
missing_summary <- function(data) {
  count <- data %>% summarise_each(funs(sum(is.na(.))))
  output <- data.frame(variable = names(count), missing_count = t(count))
  rownames(output) <- 1:nrow(output)
  output <- output[output$missing_count > 0, ]
  return(output)
}

missing <- missing_summary(data[, !names(data) %in% c("classe") ])
render_table(missing)
```

<br/>
Quite a bit of missing data here. We will remove most of these columns in the next section.

# Data Cleanup

The headlines surveyed are the following:

```{r, results="asis"}
render_table(headlines)
```

<br/>
These are not available as a data-set, so I have built a file (`headlines.csv`) with this data. Given that is a reasonable amount of data, I believe that creating a `*.csv` file is cleaner than using code to create a data frame.

There are correspondent columns in the data-set for each of the headlines, that is, one for each "A", "B", "C", etc.

|Pattern                                            |Measurement|Values                                                                                            |
|---------------------------------------------------|-----------|--------------------------------------------------------------------------------------------------|
|`(LOOPDWD7_DWD8_)[A-K](_DWD7)`                     |Recall     |"yes" [I remember the headline], "no" and "unsure".                                               |
|`(LOOPDWD7_DWD8_)[A-K](_DWD8)`                     |Accuracy   |[the claim in the headline is] "somewhat accurate", "not very accurate" and "not at all accurate".|

The actual recall and accuracy values are encoded as numbers though, so follows data frames mapping each value to its correspondent code.

Recall codes:

```{r, results="asis"}
options <- c("yes", "no", "unsure")
recalls <- data.frame(id = 1:length(options), recall = options)
render_table(recalls)
```

<br/>
Accuracy codes:

```{r, results="asis"}
options <- c("very accurate", "somewhat accurate", "not very accurate", "not at all accurate")
accuracies <- data.frame(id = 1:length(options), accuracy = options)
render_table(accuracies)
```

<br/>
Let's try to analyze some columns which migh be interesting. First of all, recall, accuracy and order:

```{r}
variable_names <- function(data, pattern) names(data)[grep(pattern, names(data))]

recall_variables <- function(data) variable_names(data, "(LOOPDWD7_DWD8_)[A-K](_DWD7)")
accuracy_variables <- function(data) variable_names(data, "(LOOPDWD7_DWD8_)[A-K](_DWD8)")
```

|Variable                            |Value                                                                                                                  |
|------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
|ID                                  |User ID.                                                                                                               |
|resp_age                            |Responder's age.                                                                                                       |
|resp_gender                         |Responder's gender, male: 1 or female: 2.                                                                              |
|HCAL_REGION1_Label_US               |U.S. states, string.                                                                                                   |
|HCAL_STDREGION_4CODES_Label_US      |U.S. regions, string.                                                                                                  |
|Weightvar                           |That's the weighting adjustment, to compensate for oversampling of any group.                                          |
|DP_INCOME                           |Income bands, as per data frame defined below.                                                                         |
|DP_EDUCATION_BAN                    |Education bands, as per data frame defined below.                                                                      |
|DP_ETHNICITY_BAN                    |Ethnicity, as per data frame defined below.                                                                            |
|DWD1                                |Political Afiliation, as per data frame defined below.                                                                 |
|DWD6                                |Presidential candidate, as per data frame defined below.                                                               |
|`(GRID_DWD11_)([1-9][0-9]*)(_DWD11)`|Scores for news sources, 1-15 representing sources as per data frame defined below. 1-4 scores as per data frame below.|


```{r}
news_source_score_variables <- function(data) variable_names(data, "(GRID_DWD11_)([1-9][0-9]*)(_DWD11)")
```

Many of the variable values are encoded as numbers though, so follows data frames mapping each value to its correspondent code.

```{r, results="asis"}
options <- c("Less than $25,000", "$25,000 to $34,999", "$35,000 to $49,999", "$50,000 to $74,999", "$75,000 to $99,999", "$100,000 to $149,999", "$150,000 or more")
income_bands <- data.frame(id = 1:length(options), income = options)
render_table(income_bands)
```

```{r, results="asis"}
options <- c("Less than high school", "High school graduate (includes equivalency)", "Some college, no degree", "Associate's degree", "Bachelor's degree", "Ph.D.", "Graduate or professional degree")
education_bands <- data.frame(id = 1:length(options), education = options)
render_table(education_bands)
```

```{r, results="asis"}
options <- c("White", "Black or African American", "Other race")
ethnicities <- data.frame(id = 1:length(options), ethnicity = options)
render_table(ethnicities)
```

```{r, results="asis"}
options <- c("Democrat", "Republican", "Independent", "Other")
parties <- data.frame(id = 1:length(options), party = options)
render_table(parties)
```

```{r, results="asis"}
options <- c("Hillary Clinton", "Donald Trump", "Gary Johnson", "Jill Stein", "Other")
candidates <- data.frame(id = 1:length(options), candidate = options)
render_table(candidates)
```

```{r, results="asis"}
options <- c("Is a major source of news for me", "Is a minor source of news for me", "Is rarely a source of news for me", "Is never a source of news for me", "I am not familiar with this news source")
news_sources_scores <- data.frame(id = 1:length(options), news_source_score = options)
render_table(news_sources_scores)
```

```{r, results="asis"}
options <- c("BuzzFeed", "Huffington Post", "New York Times", "Facebook", "Twitter", "Snapchat", "VICE", "CNN", "Vox", "Business Insider", "Washington Post", "Google News", "Yahoo News", "Drudge Report", "Fox News")
news_sources <- data.frame(id = 1:length(options), news_source = options)
render_table(news_sources)
```

<br/>
I'm going to start by removing columns that are not required at the moment:

```{r}
column_index <- function(name, data) {
  grep(paste0("^", name, "$"), colnames(data))
}

demographic_variables <- c("resp_age", "HCAL_REGION1_Label_US", "HCAL_STDREGION_4CODES_Label_US", "DP_INCOME", "DP_EDUCATION_BAN", "DP_ETHNICITY_BAN", "DWD1", "DWD6")

useful_columns <- c("ID",
                     "Weightvar",
                     demographic_variables,
                     recall_variables(data),
                     accuracy_variables(data),
                     news_source_score_variables(data))


data <- data[, useful_columns]
```

I'm going to decode these variable names and values (from the numeric code to description string) so they are easier to read:

```{r}
create_key_value_pairs <- function(keys, values) {
  pairs <- values
  names(pairs) <- keys
  return(pairs)
}

decode_column <- function(data, codes, column) {
  joint <- create_key_value_pairs(c(column), c("id"))
  data <- left_join(data, codes, by = joint)
  data <- data[, ! names(data) %in% c(column)]
  return(data)
}

data <- decode_column(data, income_bands, "DP_INCOME")
data <- decode_column(data, education_bands, "DP_EDUCATION_BAN")
data <- decode_column(data, ethnicities, "DP_ETHNICITY_BAN")
data <- decode_column(data, parties, "DWD1")
data <- decode_column(data, candidates, "DWD6")
```

Some of the variables only require a change of name, decoding not necessary:

```{r}
data <- plyr::rename(data, c("resp_age"="age", "HCAL_REGION1_Label_US"="state", "HCAL_STDREGION_4CODES_Label_US"="region"))
demographic_variables <- c("age", "state", "region", "income", "ethnicity", "education", "party", "candidate")

recall_variables <- function(data) variable_names(data, "recall_[A-K]")
accuracy_variables <- function(data) variable_names(data, "accuracy_[A-K]")
```

Recalls, accuracies and news sources are a bit more complicated case, so I have created functions with the purpose of decoding their values and renaming them.

First recall:

```{r}
rename_columns <- function(data, from_names, to_names) {
  renaming <- create_key_value_pairs(from_names, to_names)
  data <- plyr::rename(data, renaming)
  return(data)
}

rename_headline_column <- function(data, from_name, to_prefix, headline_code) {
  to_column <- paste0(to_prefix, "_", headline_code)
  data <- rename_columns(data, c(to_prefix), c(to_column))
  data <- data[, ! names(data) %in% c(from_name)]
  return(data)
}

rebuild_column <- function(data, codes, headline_code, prefix, suffix, to_prefix) {
  column <- paste0(prefix, headline_code, suffix)
  joint <- create_key_value_pairs(c(column), c("id"))
  full_join(data, recalls, by = joint)
  data <- full_join(data, codes, by = joint)
  data <- rename_headline_column(data, column, to_prefix, headline_code)
  return(data)
}

rebuild_recall_columns <- function(data) {
  for (headline_code in headlines$headline_id) {
    data <- rebuild_column(data = data, codes = recalls, headline_code, prefix = "LOOPDWD7_DWD8_", suffix = "_DWD7", to_prefix = "recall")
  }
  return(data)
}

data <- rebuild_recall_columns(data)
```

Then accuracy:

```{r}
rebuild_accuracy_columns <- function(data) {
  for (headline_code in headlines$headline_id) {
    data <- rebuild_column(data = data, codes = accuracies, headline_code, prefix = "LOOPDWD7_DWD8_", suffix = "_DWD8", to_prefix = "accuracy")
  }
  return(data)
}

data <- rebuild_accuracy_columns(data)
```

Then news sources:

```{r}
compute_news_source_variable_name <- function(string) paste0("score_", str_replace_all(string, "[^A-Za-z]", "_"))

rename_news_source_column <- function(data, news_sources, column, news_source_variable_index) {
  news_source_variable_name <- compute_news_source_variable_name(news_sources$news_source[news_source_variable_index])
  data <- rename_columns(data, c("news_source_score"), c(news_source_variable_name))
  data <- data[, ! names(data) %in% c(column)]
  return(data)
}

rebuild_news_source_column <- function(data, news_source_variable_index) {
  column <- paste0("GRID_DWD11_", news_source_variable_index, "_DWD11")
  joint <- create_key_value_pairs(c(column), c("id"))
  data <- full_join(data, news_sources_scores, by = joint)
  data <- rename_news_source_column(data, news_sources, column, news_source_variable_index)
  return(data)
}

rebuild_news_source_columns <- function(data) {
  for(index in news_sources$id) {
    data <- rebuild_news_source_column(data, index)
  }
  return(data)
}

data <- rebuild_news_source_columns(data)
```

# All Cleaned-up Data-set

Should be much more readable now. In the next sections we will compute the statistics and build our leaflet map.

## Taking a Sample

Let's now take a sample from the data-set and see what it looks like:

```{r, results = "asis"}
render_table(sample_data_frame(data, 6))
```

<br/>
Here a look at the variables in the data-set if you didn't bother to go through the data cleanup section:

```{r}
str(data)
```

# Breaking Recall, Accuracy & News Sources Scores

For convinience, I'm going to break the original data into three, for recall, accuracy, and news souce score.

## Recall & Accuracy

```{r}
extract_headline <- function(name, headlines) return(str_match_all(name, "(accuracy|recall)_([A-K])")[[1]][3])

recall_data <- data[, c("ID", "Weightvar", demographic_variables, recall_variables(data))]
accuracy_data <- data[, c("ID", "Weightvar", demographic_variables, accuracy_variables(data))]

rebuild_column <- function(data, column) {
  data <- melt(data, id.vars=c("ID", "Weightvar", demographic_variables))
  data$variable <- sapply(data$variable, extract_headline)
  data <- rename_columns(data, c("variable", "value"), c("headline_id", column))
  return(data)
}

accuracy_data <- rebuild_column(accuracy_data, "accuracy")
recall_data <- rebuild_column(recall_data, "recall")
```

For simplicity, I'm going to generate a binary variable for the responder guessing the headline veracity correctly or incorrectly:

```{r}

add_correct_answer <- function(data) {
  data$guessed_correctly <- ifelse(data$headline_status == "Fake" & grepl("not", data$accuracy) | 
                                   data$headline_status == "Real" & ! grepl("not", data$accuracy),
                                   "yes", "no")
  return(data)
}

accuracy_data <- left_join(accuracy_data, headlines, by = "headline_id")
accuracy_data <- add_correct_answer(accuracy_data)
```

The same for the responder having heard the headline:

```{r}
add_seen_fake_headline <- function(data) {
  data$seen_fake_headline <- ifelse(data$headline_status == "Fake" & grepl("yes", data$recall) ,"yes", "no")
  data[data$headline_status == "Real", "seen_fake_headline"] <- NA
  return(data)
}

recall_data <- left_join(recall_data, headlines, by = "headline_id")
recall_data <- add_seen_fake_headline(recall_data)
```

## News source

```{r}
news_source_score_variables <- function(data) variable_names(data, "score_.+")

extract_news_source <- function(name) {
  name <- str_match_all(name, "score_(.+)")[[1]][2]
  name <- str_replace_all(name, "_", " ")
  return(name)
}

rebuild_column <- function(data) {
  data <- data[, c("ID", "Weightvar", demographic_variables, news_source_score_variables(data))]
  data <- melt(data, id.vars=c("ID", "Weightvar", demographic_variables))
  data$variable <- sapply(data$variable, extract_news_source)
  data <- rename_columns(data, c("variable", "value"), c("news_source", "news_source_score"))
  return(data)
}

news_source_score_data <- rebuild_column(data)
```

# Computing Statistics

## Accuracy

```{r}
compute_overall_percentages <- function(data) {
  count_column <- "guessed_correctly"
  counts <- group_by_(data, .dots = c("state", count_column)) %>% summarise(freq = sum(Weightvar))
  percentages <- group_by_(counts, .dots = c("state")) %>% dplyr::mutate(percentage = round(freq * 100 / sum(freq), 2))
  percentages <- percentages[percentages[[count_column]] == "no", ]
  return(percentages)
}

compute_percentages <- function(data, column) {
  count_column <- "guessed_correctly"
  data <- data[!is.na(data[, column]), ]
  counts <- plyr::count(data, vars = c("state", column, count_column))
  percentages <- group_by_(counts, .dots = c("state", column)) %>% dplyr::mutate(percentage = round(freq * 100 / sum(freq), 2))
  percentages <- percentages[percentages[[count_column]] == "no", ]
  return(percentages)
}

overall_percentages <- compute_overall_percentages(accuracy_data)
education_percentages <- compute_percentages(accuracy_data, "education")
party_percentages <- compute_percentages(accuracy_data, "party")
candidate_percentages <- compute_percentages(accuracy_data, "candidate")
income_percentages <- compute_percentages(accuracy_data, "income")
ethnicity_percentages <- compute_percentages(accuracy_data, "ethnicity")
```

## News Source

For simplicity, I'm only going calculate the percentages for major source of news:

```{r}
counts <- group_by_(news_source_score_data[news_source_score_data$news_source_score == "Is a major source of news for me", ],
                    .dots = c("state", "news_source")) %>% summarise(freq = sum(Weightvar))
news_sources_percentages <- group_by_(counts, .dots = c("state")) %>% dplyr::mutate(percentage = round(freq * 100 / sum(freq), 2))
```

# Adding Geo-coordinates

First of all, we need to add shape coordinates for the states, so we can create polygons for them:

```{r}
states <- shapefile("../input/cb_2015_us_state_20m/cb_2015_us_state_20m.shp")
```

Second, I'm going to add an ID for color, so each state polygon is assigned a different color:

```{r}
states$color <- 1:nrow(states)
```

Finally, we need to add coordinates for the center of each state for markers:

```{r}
states_centers <- read.csv("../input/states_centers.csv", na.strings=c("NA", "NULL", ""), stringsAsFactors = FALSE)
```

# Graphical Exploration <a name="graphicalexploration"></a>

The leaflet plot in this section shows the percentage of people, who asked about the headlines below, could not tell which ones were real or fake:

```{r, results="asis"}
render_table(headlines)
```

<br/>
If you click a particular U.S. state, you should see a popup with the percentage of times people answered a fake headline was real or the other way around.

The following code create HTML popups for each state:

```{r}
build_overall_popup <- function(data, state){
  state_data <- data[data$state == state, ]
  content <- paste0("<h3>", state, " (overall)</h3>")
  content <- paste0(content, "<p>", state_data$percentage, "%</p>")
  return(content)
}

build_column_popup <- function(data, column, state){
  state_data <- data[data$state == state, ]
  content <- paste0("<h3>", state, "</h3>")
  column_values <- unique(state_data[[column]])
  for(column_value in column_values) {
    content <- paste0(content, "<p>", column_value, ": ", state_data$percentage[state_data[[column]] == column_value], "%")
  }
  return(content)
}

states$overall_popup <-sapply(states$NAME, build_overall_popup, data = overall_percentages)
states$education_popup <-sapply(states$NAME, build_column_popup, data = education_percentages, column = "education")
states$party_popup <-sapply(states$NAME, build_column_popup, data = party_percentages, column = "party")
states$candidate_popup <-sapply(states$NAME, build_column_popup, data = candidate_percentages, column = "candidate")
states$income_popup <-sapply(states$NAME, build_column_popup, data = income_percentages, column = "income")
states$ethnicity_popup <-sapply(states$NAME, build_column_popup, data = ethnicity_percentages, column = "ethnicity")
states$news_sources_popup <-sapply(states$NAME, build_column_popup, data = news_source_percentages, column = "news_source")
```

Finally, without much ado, here's the interactive plot:

```{r}
add_polygons <- function (map, states, column, group) {
  map %>% addPolygons(
    stroke = TRUE, weight = 2, fillOpacity = 0.3, smoothFactor = 0.1,
    color = "white",
    fillColor = ~colorNumeric("Paired", sample(states$color))(color),
    popup = states[[column]], group = group
  )
}

usa_longitude = -95.7129
usa_latitude = 37.0902

leaflet(states, width = "100%") %>%
  setView(usa_longitude, usa_latitude, zoom = 4) %>%
  
  addTiles() %>%
  
  add_polygons(states, "news_sources_popup", "Major News Source") %>%
  
  add_polygons(states, "overall_popup", "Tricked Overall") %>%
  add_polygons(states, "education_popup", "Tricked by Education") %>%
  add_polygons(states, "party_popup", "Tricked by Party") %>%
  add_polygons(states, "candidate_popup", "Tricked by Candidate") %>%
  add_polygons(states, "income_popup", "Tricked by Income") %>%
  add_polygons(states, "ethnicity_popup", "Tricked by Ethnicity") %>%

  addLayersControl(
    baseGroups = c("Major News Source", "Tricked Overall", "Tricked by Education", "Tricked by Party", "Tricked by Candidate", "Tricked by Income", "Tricked by Ethnicity"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  showGroup(group = "Tricked Overall")
```


Choose the type of statistics you want to see by clicking the correspondent radio box and then click the state on the U.S. map to get the statistic for the state.