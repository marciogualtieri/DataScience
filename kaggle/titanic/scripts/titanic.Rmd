---
title: "Titanic Survivorship Analysis"
output:
  html_notebook:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: 4
---

# Overview

I intend to use this notebook to showcase my data science and R skills. I'm trying to be verbose for this reason. This notebook also serves as my "bag of tricks", which I can consult in future projects. Keep in mind that I'm a coder by trade in my way to become a data scientist. Suggestions and constructive critique are always welcome.

# Installing the Required Packages

You might need to install the following packages if you don't already have them:

```{r}
# install.packages("xtable")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("plyr")
# install.packages("vcd")
# install.packages("png")
# install.packages("caret")
# install.packages("randomForest")
# install.packages("ggrepel")
# install.packages("party")
# install.packages("Amelia")
# install.packages("mice")
# install.packages("mlbench")
# install.packages("party")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("rattle")
# install.packages("ROCR")
# install.packages("Hmisc")
```

Just uncomment the packages you need and run this chunk before you run the remaining ones in this notebook.

# Importing the Required Packages

Once the libraries are installed, you need to load them as follows:

```{r}
suppressMessages(library(caret))                                 # Machine learning
suppressMessages(library(rpart))
suppressMessages(library(e1071))
suppressMessages(library(randomForest))
suppressMessages(library(mlbench))
suppressMessages(library(party))
suppressMessages(library(ggplot2))                               # Plotting
suppressMessages(library(lattice))
suppressMessages(library(grid))
suppressMessages(library(gridExtra, warn.conflicts = FALSE))
suppressMessages(library(ggrepel))
suppressMessages(library(vcd))
suppressMessages(library(rpart.plot))
suppressMessages(library(rattle))
suppressMessages(library(ROCR))
suppressMessages(library(mice))                                  # Data imputation
suppressMessages(library(xtable))                                # Pretty printing dataframes
suppressMessages(library(plyr, warn.conflicts = FALSE))          # Manipulating dataframes
suppressMessages(library(Hmisc))
suppressMessages(library(Amelia))                                # Missing data
suppressMessages(library(dplyr, warn.conflicts = FALSE))
suppressMessages(library(stringr))                               # String operations
```

# Loading Data

## Reading CSV Files

Let's first load the data-sets:

```{r}
train <- read.csv('../input/train.csv', na.strings=c("NA", "NULL", ""), stringsAsFactors = F)
test  <- read.csv('../input/test.csv', na.strings=c("NA", "NULL", ""), stringsAsFactors = F)
all <- rbind.fill(train, test)
```

You might have noticed that the testing data-set isn't labeled (`Survived` is not present):

```{r}
str(test)
```

Kaggle only provides labels for the training data-set. The testing data-set is using to compute your score (based on accuracy).

## Record Counting

Here's the record count for the data-sets:

```{r}
trainRows <- nrow(train)
testRows <- nrow(test)
totalRows <- nrow(train) + nrow(test)
```

|Data-set|Number of Records|
|-------|------------------|
|train  |`r trainRows`     |
|test   |`r testRows`      |
|total  |`r totalRows`     |

## Taking a Sample

The standard way data is rendered in a notebook shows nicely in RStudio, but look terrible in the HTML output (which is probably the format you're reading this right now), so I created a custom function for this purpose:

```{r}
renderTableInViewerPane <- function(data) {
  html <- print(xtable(data), type = "html", print.results=FALSE)
  temp <- tempfile(fileext = ".html")
  cat(html, file = temp)
  rstudioapi::viewer(temp)
}

renderTable <- function(data) {
  renderTableInViewerPane(data)
  print(xtable(data), type = "html")
}

sampleDataFrame <- function(data, size) {
  sampleIndex <- sample(1:nrow(train), size)
  return(data[sampleIndex, ])
}
```

Here are some records sampled from the training data-set:


```{r, results="asis"}
renderTable(sampleDataFrame(train, 10))
```

## Data Types

You'll find a description of the variables [here](https://www.kaggle.com/c/titanic/data), but in the spirit of making this notebook self-contained:

</br>

| Name    |Description                      |Type                                                                    |
|---------|---------------------------------|------------------------------------------------------------------------|
|Pclass	  |Passenger’s class                |Numeric / Categorical (1 = 1st class; 2 = 2nd class; 3 = 3rd class)     |
|Name	    |Passenger’s name                 |String                                                                  |
|Sex	    |Passenger’s gender               |String / Categorical (male, female)                                     |
|Age	    |Passenger’s age                  |Numeric                                                                 |
|SibSp	  |Number of siblings/spouses aboard|Numeric                                                                 |
|Parch	  |Number of parents/children aboard|Numeric                                                                 |
|Ticket	  |Ticket Code                      |Alpha-numeric                                                           |
|Fare	    |Fare                             |Numeric                                                                 |
|Cabin	  |Cabin Code                       |Alpha-numeric                                                           |
|Embarked	|Port of embarkation              |Character / Categorical (C = Cherbourg; Q = Queenstown; S = Southampton)|

`read.csv()` will assign types automatically. You can always check your data types evoking `str()`:

```{r}
str(train)
```

# Missing Data

## Detecting Missing Data

It's always a good idea to know about missing values from the start:

```{r, fig.width = 12, fig.height = 12}
missmap(train, main = "Missing Values (Training Data-set)", col = c("red", "lightgrey"))
```


```{r, fig.width = 12, fig.height = 12}
missmap(test, main = "Missing Values (Testing Data-set)", col = c("red", "lightgrey"))
```

```{r}
missingCabinRows <- nrow(all[is.na(all$Cabin), ])
missingAgeRows <- nrow(all[is.na(all$Age), ])
missingFareRows <- nrow(all[is.na(all$Fare), ])
missingEmbarkedRows <- nrow(all[is.na(all$Embarked), ])
```

`Cabin`, `Age` and `Embarked` are missing for a number of records:

|Variable  |Number of Missing Records|
|----------|-------------------------|
|`Cabin`   |`r missingCabinRows`     |
|`Age`     |`r missingAgeRows`       |
|`Fare`     |`r missingFareRows`     |
|`Embarked`|`r missingEmbarkedRows`  |

<br/>
The classifier we'll create won't work well with missing data (they either won't work altogether or you will get NAs as predictions), so we need to estimate missing values using [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) methods.

## Dealing with Missing Data

### Missing `Age` and `Fare`

For continuous variables such as these [mice](https://cran.r-project.org/web/packages/mice/index.html) is a good choice for imputation:

```{r}
estimateMissingVariables <- function(data) {
  predictors <- c("Age", "Sex", "Fare", "Pclass", "SibSp", "Parch", "Embarked", "Title")
  set.seed(345)
  capture.output(model <- mice(data[, names(data) %in% predictors], method='rf'))
  output <- complete(model)
  data$Age <- output$Age
  data$Fare <- output$Fare
  return(data)
}

fixedAll <- estimateMissingVariables(all)
```

Let's do a quick sanity check by comparing histograms before and after estimating the missing values. I have created a custom histogram for this purpose:

```{r}
customHistogram <- function(data, column, title) {
  missing = nrow(data[is.na(data[,  column]), ])
  ggplot(data=data, aes_string(x = column)) +
  geom_histogram(bins = 20, na.rm = TRUE, fill = "blue", alpha = 0.2) +
  xlab(paste(column, "(", title, ", NA Count: ", missing, ")"))
}
```

Which renders the following plots:

```{r, fig.width = 12, fig.height = 12}
allAge <- customHistogram(all, "Age", "Original")
fixedAllAge <- customHistogram(fixedAll, "Age", "Fixed")

allFare <- customHistogram(all, "Fare", "Original")
fixedAllFare <- customHistogram(fixedAll, "Fare", "Fixed")

grid.arrange(allAge, fixedAllAge, 
             allFare, fixedAllFare,
             ncol=2, nrow=2)
```

No NAs and overall shape didn't change much. Looking good, so let's use them:

```{r}
all <- fixedAll
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

### Missing `Embarked`

These are the records missing `Embarked`:

```{r missingEmbarked, results='asis'}
missingEmbarkedIndex <- is.na(train$Embarked)
renderTable(train[missingEmbarkedIndex, ])
```

<br/>
Given that `Embarked` is categorical, we could use random forest here, but just to try something different, let's try a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) from the [rpart](https://cran.r-project.org/web/packages/rpart/index.html) package:

```{r}
estimateMissingEmbarked <- function(data) {
  missing <- data[is.na(data$Embarked), ]
  present <- data[!is.na(data$Embarked), ]

  fol <- formula(Embarked ~ Sex + Age + Fare + Pclass + SibSp + Parch)
  model <- rpart(fol, method='class', data=present)
  missing$Embarked <-predict(model, missing, type="class")
  all <- rbind.fill(missing, present)
  all <- all[with(all, order(PassengerId)), ]
  return(all)
}

all <- estimateMissingEmbarked(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

These are the predictions for `Embarked`:

```{r fixedEmbarked, results='asis'}
renderTable(train[missingEmbarkedIndex, ])
```

<br/>
In the section for [`Fare` Analysis](#fareanalysis) that will follow, we will be able to sanity check these estimations: the distribution of `Fare` can show us that the estimates make sense.

# Feature Engineering

## Converting Categorical Fields to Factors

The categorical fields need to be converted to factors (which are well-suited for categorical data) for tasks such as plotting and machine learning:

```{r}
toFactor <- function(data) {
   columns <- intersect(names(data), c("Survived", "Sex", "Embarked", "Pclass", "Ticket"))
  data[, columns] <- lapply(data[, columns] , factor)
  return(data)
}

all <- toFactor(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Note that these columns are now factors:

```{r}
str(train)
```

> **NOTE:**
> When imputating missing values or creating new features, I suggest that you always apply such transformations over the whole data (training + testing) if you're working with categorical variables. Let's say that you have a factor named `Pie` with levels `Apple`, `Pecan`, `Cherry` and `Pumpkim`. Could be the case that your training data-set has only entries with levels `Apple` and `Pecan` and that your testing data-set has levels `Cherry` and `Pumpkim`. On factor conversion, `Pie` from training will have different levels from `Pie` from testing (they are incompatible). This will be a pain when you're ready to train your classifier. Of course, you can always override variable factor levels for both data-sets afterwards, but it's a somehow ackward operation.

## `Name` Analysis

### Extracting `Surname` from `Name`

Let's see if we can improve accuracy by squeezing some more information from the remaining variables, starting by `Name`. Here's a sample from the training data-set:

```{r}
sample <- head(train, 12)
sample$Name
```

The pattern seem to be "Surname, Title. Names". We're particularly interested in "Surname" and "Title". I have extracted parsing into functions for cleaner code:

```{r}
namePattern <- "(.+),\\s*(.+?)\\..+"

extractSurname <- function(name) {
  return(str_match_all(name, namePattern)[[1]][2])
}

addSurname <- function(data) {
  data$Surname <- sapply(data$Name, extractSurname)
  data[, "Surname"] <- as.factor(data[, "Surname"])
  return(data)
}

all <- addSurname(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

### Extracting `Title` from `Name`

```{r}
extractTitle <- function(name) {
  return(str_match_all(name, namePattern)[[1]][3])
}

addTitle <- function(data) {
  data$Title <- sapply(data$Name, extractTitle)
  data[, "Title"] <- as.factor(data[, "Title"])
  return(data)
}

all <- addTitle(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Here's the distribution for `Title`:

```{r, fig.width = 12, fig.height = 12}
countBarchart <- function(data, column, title) {
   ggplot(data, aes_string(x=column)) + 
   geom_bar(fill = "blue", alpha = 0.2) +
   geom_text(stat='count', aes(label=sprintf("%d\n(%d %%)", ..count.., round(..count.. * 100/sum(..count..), 0)), vjust=0)) +
   xlab(paste(column, "(", title, ")"))
}

countBarchart(train, "Title", "Overall")
```

#### Outliers

The most frequent titles are "Mr", "Miss", "Mrs" and "Master". The remaining ones seem to be outliers. Normally, one would simply remove these, but in the spirit of showing the effect of outliers, let's create a new column: `TitleWO` (without outliers):

```{r}
addTitleWO <- function(data) {
  frequent <- c("Mr", "Miss", "Mrs", "Master")
  data$TitleWO <- sapply(data$Name, extractTitle)
  data$TitleWO[!(data$Title %in% frequent)] <- "Rare"
  data[, "TitleWO"] <- as.factor(data[, "TitleWO"])
  return(data)
}

all <- addTitleWO(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]

countBarchart(train, "TitleWO", "Overall")
```

<br/>
There are different ways to visualize survival rates, but I think that bar charts are the best tool for this purpose:

```{r, fig.width = 12, fig.height = 12}
categoricalResultCountBarchart <- function(data, column, categoryColumn) {
  survivors <- plyr::count(data, vars=c(column, categoryColumn))
  survivors <- group_by_(survivors, column) %>% dplyr::mutate(Percentage = round(freq * 100 / sum(freq)))

  ggplot(data = survivors, aes_string(x = column, y = "Percentage", fill = categoryColumn)) +
    geom_bar(stat="identity", position = "dodge") +
    geom_text(aes(label=sprintf("%d\n(%d %%)", freq, Percentage)))
}

categoricalResultCountBarchart(train, "TitleWO", "Survived")
```

> **NOTE:**
> You might have noticed that the bar heights represent percentages within a given group, not record counts. That's because we might not get record counts of the same order of magnitude for every group, i.e., a large number of "Mr" and a small number of "Ms", which would render the bar for the last nearly invisible. I have labeled the bars with the record count and percentage though, so the reader is informed of the actual count.

## Women and Children First

Let's start with the first thing that comes to mind in a moment of crisis, that known code of conduct: ["Women and children first."](https://en.wikipedia.org/wiki/Women_and_children_first).

![["Women and children first? Really? Shouldn't be whoever's closest?"](https://www.youtube.com/watch?v=yL2dGTDQXVo)](https://s5.postimg.org/8zzl5ot53/larry_women_and_children_first.png)

### Gender Survivorship

```{r}
categoricalResultCountBarchart(train, "Sex", "Survived")
```

It's clear that there is a correlation between gender and survivorship.

### Age Survivorship

Once again I have created a custom histogram plot function, which shows percentages instead of absolute values:

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram <- function(data, column, categoryColumn, breaks) {
  groupColumn <- paste0(column, "Group")
  suppressWarnings(data[, groupColumn] <- cut2(data[, column], g=breaks, digits=0))
  survivors <- plyr::count(data, vars=c(groupColumn, categoryColumn))
  survivors <- group_by_(survivors, groupColumn) %>% dplyr::mutate(Percentage = round(freq * 100 / sum(freq)))

  ggplot(data = survivors, aes_string(x = groupColumn, y = "Percentage", fill = categoryColumn)) +
    geom_bar(stat="identity", position = "dodge") +
    geom_text(aes(label=sprintf("%d\n(%d %%)", freq, Percentage))) +
    xlab(column)
}

categoricalResultHistogram(train, "Age", "Survived", 10)
```

<br/>
Younger people (less than 12 years old) does seem to have a higher chance of surviving.

#### Outliers

Let's take this opportunity to develop some tools that will help us to analyze other variables for which the outliers might not be so obvious:

```{r, fig.width = 6, fig.height = 12}
createBoxPlotLabels <- function(data, column) {
  meta <- boxplot.stats(data[, column])
  labels <-data.frame(value=round(median(data[, column]), 4), label="Median")
  labels <-rbind(labels,data.frame(value=round(mean(data[, column]), 4), label="Mean"))
  labels <-rbind(labels,data.frame(value=meta$stats[2], label="1st Quartile"))
  labels <-rbind(labels,data.frame(value=meta$stats[4], label="3rd Quartile"))
  if(length(meta$out) > 0) labels <-rbind(labels,data.frame(value=round(median(meta$out), 4), label="Outliers Median"))
  if(length(meta$out) > 0) labels <-rbind(labels,data.frame(value=round(mean(meta$out), 4), label="Outliers Mean"))
  return(labels)
}

customBoxPlot <- function(data, column, title) {
  labels <- createBoxPlotLabels(data, column)
  ggplot(data, aes_string(x="factor(0)", y=column)) +
  geom_boxplot(fill = "blue", alpha=0.2) +
  geom_point(position = position_jitter(width = 0.2), color = "darkblue") +
  geom_label_repel(data=labels,
                   aes(x=factor(0), y=value, label=paste0(label, ": ", value)),
                   colour="red", angle=0, size=3,
                   point.padding = unit(1.0, "lines"), box.padding = unit(0.5, "lines")) +
  xlab(title)
}

customBoxPlot(all, "Age", "Overall")
```

I'm going to remove the outliers by replacing them with the median:

```{r}
quarter1Indexes <- function(data, column) {
  meta <- boxplot.stats(data[, column])
  q1 <- meta$stats[2]
  return(which(data[, column] < q1))
}

quarter3Indexes <- function(data, column) {
  meta <- boxplot.stats(data[, column])
  q3 <- meta$stats[4]
  return(which( data[, column] > q3))
}

outliersMedian <- function(data, column) {
  meta <- boxplot.stats(data[, column])
  return(median(meta$out))
}

addAgeWO <- function(data) {
  data$AgeWO <- data$Age
  q1Median <- median(data$Age[quarter1Indexes(data, "Age")])
  q3Median <- median(data$Age[quarter3Indexes(data, "Age")])
  data$AgeWO[quarter1Indexes(data, "Age")] <- q1Median
  data$AgeWO[quarter3Indexes(data, "Age")] <- q3Median
  return(data)
}

all <- addAgeWO(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Let's see how it looks like without the outliers:

```{r, fig.width = 12, fig.height = 12}
ageHistogram <- customHistogram(all, "Age", "Overall")
ageWOHistogram <- customHistogram(all, "AgeWO", "Overall")

grid.arrange(ageHistogram, ageWOHistogram, ncol = 2)
```

Let's look at the survivorship again after removing the outliers:

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(train, "AgeWO", "Survived", 10)
```

It didn't work so well, so let's try to define the outliers manually based on the trends we have seen in the histogram:

```{r}
addAgeWO <- function(data) {
  data$AgeWO <- data$Age
  data$AgeWO[data$Age <13] <- 12
  q3Median <- median(data$Age[quarter3Indexes(data, "Age")])
  data$AgeWO[quarter3Indexes(data, "Age")] <- q3Median
  return(data)
}

all <- addAgeWO(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Let's see how it looks like:

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(train, "AgeWO", "Survived", 10)
```

Looks better, we have preserved the trend we have seen for young people.

#### Checking for Pubes

![["Are you kidding me? That's not a kid! That's a midget! I can tell!""]((https://www.youtube.com/watch?v=yL2dGTDQXVo))](https://s5.postimg.org/yw8my032f/larry_thats_a_midiget.png)

<br/>
Let's try to go all the way up to eleven and break `Age` into two categories: `IsChild`equals TRUE and `IsChild` equals FALSE:

```{r}
addIsChild <- function(data) {
  data$IsChild <- data$Age < 12
  data[, "IsChild"] <- as.factor(data[, "IsChild"])
  return(data)
}

all <- addIsChild(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]

categoricalResultCountBarchart(train, "IsChild", "Survived")
```

### Conclusion

Children do seem to have a greater chance of surviving (as did women).

## Survival of the Richest

### `Fare` Analysis <a name="fareanalysis"></a>

Let's start with `Fare` as a measure of wealth. Once again I have created a custom plot, this time a density plot:

```{r}
verticalLine <- function(value, name, color) {
  label <- paste("paste(", shQuote(name), ", ", shQuote("\\n"), ", ", value, ")")
  geomObj <- list(geom_vline(aes_string(xintercept=value), colour=color, linetype='dashed'),
                  geom_text(aes_string(x=value, y = 0, label=label)))
  return(geomObj)
}

customDensityPlot <- function(data, xColumn, title) {
  minXColumn <- paste("min(", xColumn, ", na.rm = TRUE)")
  medianXColumn <- paste("median(", xColumn, ", na.rm = TRUE)")
  maxXColumn <- paste("max(", xColumn, ", na.rm = TRUE)")
  
  ggplot(data, aes_string(x = xColumn)) + 
  geom_density(na.rm = TRUE, fill = "blue", alpha=0.2) +
  xlab(paste(xColumn, "(", title, ")")) +

  verticalLine(minXColumn,  "min", "blue") +
  verticalLine(medianXColumn, "median", "red") +
  verticalLine(maxXColumn, "max", "blue")
}
```

`Fare` depends on `Pclass` (passenger's class) and `Embarked` (location where the passenger embarked), but let's start with the overall `Fare`:

```{r, fig.width = 12, fig.height = 12}
customDensityPlot(all, "Fare", "Overall")
```

Breaking the data into all possible combinations of`Embarked` and `Pclass` might be more insightful:

```{r}
cData <- all[all$Embarked == "C", ]
c1Data <- cData[cData$Pclass == 1, ]
c2Data <- cData[cData$Pclass == 2, ]
c3Data <- cData[cData$Pclass == 3, ]

sData <- all[all$Embarked == "S", ]
s1Data <- sData[sData$Pclass == 1, ]
s2Data <- sData[sData$Pclass == 2, ]
s3Data <- sData[sData$Pclass == 3, ]

qData <- all[all$Embarked == "Q", ]
q1Data <- qData[qData$Pclass == 1, ]
q2Data <- qData[qData$Pclass == 2, ]
q3Data <- qData[qData$Pclass == 3, ]
```

```{r}
c1Density <- customDensityPlot(c1Data, "Fare", "1st-class, C")
c2Density <- customDensityPlot(c2Data, "Fare", "2nd-class, C")
c3Density <- customDensityPlot(c3Data, "Fare", "3rd-class, C")

s1Density <- customDensityPlot(s1Data, "Fare", "1st-class, S")
s2Density <- customDensityPlot(s2Data, "Fare", "2nd-class, S")
s3Density <- customDensityPlot(s3Data, "Fare", "3rd-class, S")

q1Density <- customDensityPlot(q1Data, "Fare", "1st-class, Q")
q2Density <- customDensityPlot(q2Data, "Fare", "2nd-class, Q")
q3Density <- customDensityPlot(q3Data, "Fare", "3rd-class, Q")
```

```{r, fig.width = 12, fig.height = 12}
grid.arrange(c1Density, s1Density, q1Density, 
             c2Density, s2Density, q2Density, 
             c3Density, s3Density, q3Density, 
             ncol=3, nrow=3)
```

I would say that we could use`Fare` and `Pclass` to deduce `Embarked`. These are the records missing `Embarked` in the training data-set:

```{r embarkedAnalysis, results='asis'}
renderTable(train[missingEmbarkedIndex, ])
```

<br/>
The passengers missing `Embarked` are the ones with `PassengerId` equals to 62 and 830. Note that these records are from 1st-class and their `Fare` are both equal to 80.
The density plot for "C" and 1st-class has its peak around a median of 78.667, thus "C" for the records missing `Embarked` as estimated previously using a decision tree seems reasonable.

Finally let's analyse survivorship:

Sadly, it seems like wealth increases ones chances of surviving:

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(train, "Fare", "Survived", 6)
```

#### Outliers

Once again we can use the tools we developed for checking outliers when analyzing `Age`:

```{r, fig.width = 6, fig.height = 12}
customBoxPlot(all, "Fare", "Overall")
```

Let's again remove the outliers by replacing them with the median as we did for `Age`:

```{r}
addFareWO <- function(data) {
  data$FareWO <- data$Fare
  q1Median <- median(data$Age[quarter1Indexes(data, "Fare")])
  q3Median <- median(data$Age[quarter3Indexes(data, "Fare")])
  data$FareWO[quarter1Indexes(data, "Fare")] <- q1Median
  data$FareWO[quarter3Indexes(data, "Fare")] <- q3Median
  return(data)
}

all <- addFareWO(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```


```{r, fig.width = 12, fig.height = 12}
fareHistogram <- customHistogram(all, "Fare", "Overall")
fareWOHistogram <- customHistogram(all, "FareWO", "Overall")

grid.arrange(fareHistogram, fareWOHistogram, ncol = 2)
```

Let's analyse survivorship after removing the outliers:

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(train, "FareWO", "Survived", 6)
```

<br/>
We can see that the trends we have observed before are move evident now.

### `Pclass` (Passenger's Class) Analysis

How about `Pclass` (passenger's class) as another indicator of wealth?

```{r}
categoricalResultCountBarchart(train, "Pclass", "Survived")
```

Passenger class also seem to support that wealth increases one's chance of surviving...

### Conclusion

Wealth seems to increase survivorship for both available indicators (`Fare` and `Pclass`).

![["I'm so sick of the 1% getting this preferential treatment! Enough is enough!"](https://www.youtube.com/watch?v=yL2dGTDQXVo)](https://s5.postimg.org/ie5iph9c7/bernie_and_larry.png)

## Family Ties

The next attributes are `SibSp` (passenger's siblings/spouse count) and `Parch` (passenger's parents/children count). We might be able to use these to analyse if families stick together in a disaster.

![Actual Picture of Donald Trump's Grandfather](https://s5.postimg.org/m98ws1shz/drumpf.png)

No, this gentleman isn't Trump's Grandfather. Better to be clear: Silly might [sue me](https://www.youtube.com/watch?v=WB4sGX0R5ak)!

### `SibSp` (Passenger's Siblings/Spouse count) Analysis

```{r, fig.width = 12, fig.height = 12}
categoricalResultCountBarchart(train, "SibSp", "Survived")
```

There seems to exist a sweet spot around one siblings/spouse (couples? relatives?), but it's not that clear from the plot that made a big difference.

### `Parch` (Passenger's Parents/Children) Analysis

```{r, fig.width = 12, fig.height = 12}
categoricalResultCountBarchart(train, "Parch", "Survived")
```

There also seems to be a sweet spot around one parents/children, but it's also not that clear from the plot that it makes a huge difference.

### Motherhood

One last think that comes to mind is motherhood. Would a mother with her child have better chances of surviving (a bit of a symbiotic relationship, Who would separate a mother from her child? Ok, Donald Trump would).

[This source](http://www.infoplease.com/ipa/A0005061.html) claims that women got married, in average, at an age of 21.6 years old in the 1900's. So, I'm going to consider a person over 21 years old, female and with `Parch` greater than zero a mother:

```{r}
addIsMother <- function(data) {
  data$IsMother <- data$Age > 21 & data$Sex == "female" & data$Parch > 0
  data[, "IsMother"] <- as.factor(data[, "IsMother"])
  return(data)
}

all <- addIsMother(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]

categoricalResultCountBarchart(train, "IsMother", "Survived")
```

## Location, Location, Location

### `Embarked` Analysis

```{r}
categoricalResultCountBarchart(train, "Embarked", "Survived")
```

### Conclusion

For some reason "C" (Cherbourg) seems to have higher survivorship. Let's take a look at the data:

```{r}
cData <- train[train$Embarked == "C", ]
qData <- train[train$Embarked == "Q", ]
sData <- train[train$Embarked == "S", ]
```

```{r}
cAge <- customHistogram(cData, "Age", "Embarked = C")
qAge <- customHistogram(qData, "Age", "Embarked = Q")
sAge <- customHistogram(sData, "Age", "Embarked = S")

cFare <- customHistogram(cData, "Fare", "Embarked = C")
qFare <- customHistogram(qData, "Fare", "Embarked = Q")
sFare <- customHistogram(sData, "Fare", "Embarked = S")

cSex <- countBarchart(cData, "Sex", "Embarked = C")
qSex <- countBarchart(qData, "Sex", "Embarked = Q")
sSex <- countBarchart(sData, "Sex", "Embarked = S")
```

```{r, fig.width = 12, fig.height = 12}
grid.arrange(cAge, qAge, sAge, 
             cFare, qFare, sFare, 
             cSex, qSex, sSex, 
             ncol=3, nrow=3)
```

From the plots, it seems like the population from "C" does better than the others in terms of wealth (better than "Q" and "S"), age (better than "Q" and comparable to "S") and gender (comparable to "Q" and better than "S").

## Family Ties II: Alex Keaton Travels Back in Time and kills Donald Trump

Just kidding, but wouldn't you watch this movie?

!["I'm going to build a great iceberg and keep those pesky Italians from reaching New York. Some bad uomini over there."](https://s5.postimg.org/iod18tnyf/family_ties_ii.png)

### Combining `SibSp` and `Parch` into `FamilySize`

Given that `FamilySize = SibSp + Parch + 1`, could we combine two variables into one without effect?

```{r}
addFamilySize <- function(data) {
  data$FamilySize <- data$SibSp + data$Parch + 1
  return(data)
}

all <- addFamilySize(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

### Using `Surname` to Compute Family Survival Rate

Could we somehow use `Surname`? A question we brought up earlier is: do families stick together? That is, is surviving correlated to the number of people in your family that survived?

```{r}
computeSurvivalRatePerColumn <- function(data, column) {
  rates <- plyr::count(data, vars=c(column, "Survived"))
  rates <- group_by_(rates, column) %>% dplyr::mutate(SurvivalRate = round(freq * 100 / sum(freq)))
  rates <- rates[rates$Survived == 1, ]
  rates <- rates[, which(names(rates) %in% c(column, "SurvivalRate"))]
  names(rates)[names(rates) == "SurvivalRate"] <- paste0("SurvivalRateBy", column)
  return(rates)
}

addSurvivalRate <- function(column, data, rateData) {
  rates <- computeSurvivalRatePerColumn(rateData, column)
  rateColumn <- paste0("SurvivalRateBy", column)
  
  if(rateColumn %in% names(data)) {
    data <- data[ , -which(names(data) %in% c(rateColumn))]
  }
  data <- left_join(data, rates,by=column)
  data[is.na(data[, rateColumn]), rateColumn] <- 0
  return(data)
}

all <- addSurvivalRate("Surname", all, train)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

The following distribution for `FamilySurvivalRate` seems to support that families stick together:

```{r, fig.width = 12, fig.height = 12}
countBarchart(train, "SurvivalRateBySurname", "Overall")
```

### Identifying Families

If you go to the [model evaluation results section](#aftermath2) ahead, you will find that we get a huge reduction in accuracy if we use any of the survival rates. These rates were great to visualize patterns in the data (families stick together), but not as good as predictors. Remember that these rates are merely estimates (we don't have the whole list of survivors) and therefore the estimates will be way off for families that have a small number of members. Let's try a different approach. The idea is linking people from the same family together:

```{r}
addFamilyID <- function(data) {
  data$FamilyID <- paste0(data$Surname, as.character(data$FamilySize))
  data[, "FamilyID"] <- as.factor(data[, "FamilyID"])
  return(data)
}

all <- addFamilyID(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Note that we used `Surname` and `FamilySize`. It's reasonable to assume that if people share the same `Surname` and `FamilySize` they belong together in the same family. People with equal `Surname` not necessarily are related, but using `FamilySize` reduces the chance that we are making this mistake.

#### Outliers

I'll consider a family any group of people sharing the same `Surname` with at least three people:

```{r}
MIN_FAMILY_SIZE <- 3

tooSmallFamiliesIndexes <- function(data) {
  return(which(data$FamilySize < MIN_FAMILY_SIZE))
}

addFamilyIDWO <- function(data) {
  data$FamilyIDWO <- paste0(data$Surname, as.character(data$FamilySize))
  data$FamilyIDWO[tooSmallFamiliesIndexes(data)] <- paste0("FamilySize<", toString(MIN_FAMILY_SIZE))
  data[, "FamilyIDWO"] <- as.factor(data[, "FamilyIDWO"])
  return(data)
}

all <- addFamilyIDWO(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]

all <- addSurvivalRate("FamilyIDWO", all, train)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

### Conclusion

The majority of people seem to stick together, for the best or for the worst...

![" Where do you think you're going? Nobody's leaving. Nobody's walking out on this fun, old-fashioned family Christmas. No, no. We're all in this together."](https://s5.postimg.org/9s294vxc7/griswold_family.png)

## `Cabin` Analysis

Let's take a look at a sample from `Cabin` (keeping in mind that is sparse):

```{r}
cabins <- train[!is.na(train$Cabin), ]$Cabin
head(cabins, 12)
```

I have found the following information [here](http://www.dummies.com/education/history/titanic-facts-the-layout-of-the-ship/):

<br/>

  -----------------------------------------------------------------------------------------------------------
  Deck            Fore                             Amidships                   Aft
  --------------- -------------------------------- --------------------------- ------------------------------
  Boat            Officer’s bridge (crew)          Promenade (1st)\            Promenade (2nd)
                                                   Gymnasium (1st)             

  Promenade (A)   Reading and Writing Room (1st)   Lounge (1st)                Smoking room (1st)\
                                                                               Verandah Café (1st)\
                                                                               Palm Courts (1st)

  Bridge (B)      Forecastle deck (crew)           Suites, cabins (1st)        À la Carte Restaurant (1st)\
                                                                               Café Parisien (1st)\
                                                                               Smoking room (2nd)\
                                                                               Promenade (poop deck; 3rd)

  Shelter (C)     Crew mess (crew)                 Cabins, staterooms (1st)\   Library (2nd)\
                                                   Crew mess (crew)\           Smoking room (3rd)\
                                                   Purser’s office (crew)      General room (3rd)

  Saloon (D)      Open space (3rd)\                Dining saloon (1st)\        Dining saloon (2nd)\
                  Cabins (3rd)                     Reception room (1st)\       Kitchen galleys (crew)
                                                   Cabins (1st)                

  Upper (E)       Cabins (3rd)                     Cabins (2nd)\               Cabins (2nd)\
                                                   Cabins (crew)               Cabins (3rd)

  Middle (F)      Cabins (3rd)                     Dining saloon (3rd)\        Cabins (2nd)\
                                                   Swimming pool (1st)\        Cabins (3rd)
                                                   Turkish baths (1st)         

  Lower (G)       Storage rooms (crew)\            Boiler rooms (crew)         Squash court (1st)\
                  Engine rooms (crew)                                          Post office (crew)

  Orlop           Cargo rooms (crew)\              Boiler rooms (crew)         Engine rooms (crew)\
                  Baggage rooms (crew)\                                        Cargo rooms (crew)
                  Mail room (crew)                                             

  Tank Top        Boiler rooms (crew)\             Boiler rooms (crew)\        Boiler rooms (crew)\
                  Engine rooms (crew)              Engine rooms (crew)         Engine rooms (crew)
  -----------------------------------------------------------------------------------------------------------
  

<br/>
From this table we can tell that 1st-class cabins were located in decks "C" and "D", while 2nd-class and 3rd-class were located in decks "E" and "F". In the absence of `Pclass`, `Cabin` could be a good proxy if it wasn't so sparse.
  
[Here](https://www.encyclopedia-titanica.org/titanic-deckplans/) you will find blueprints from all decks in the ship. The numbers in `Cabin` don't seem to have any correlation with survivorship, but I have found an interesting insight in [this BBC article](http://www.bbc.com/news/magazine-17515305):

<br/>  
  
> Each class of passengers had access to their own decks and allocated lifeboats - although crucially no lifeboats were stored in the third class sections of the ship.
Third class passengers had to find their way through a maze of corridors and staircases to reach the boat deck. First and second class passengers were most likely to reach the lifeboats as the boat deck was a first and second class promenade.

Which basically says that 3rd-class had some huge disadvantage regarding to reaching the boat deck. The 99% gets it once again...

```{r}
allPassengers <- nrow(train)
firstClassPercent <- round(nrow(train[train$Pclass == 1, ]) * 100 / allPassengers)
secondClassPercent <- round(nrow(train[train$Pclass == 2, ]) * 100 / allPassengers)
thirdClassPercent <- round(nrow(train[train$Pclass == 3, ]) * 100 / allPassengers)
```

Not really 99%, but `r thirdClassPercent`% though. It seems like there wasn't such an inequality in terms of number of seats (or survivors) though:

|Pclass|Percentage from Seats |
|------|----------------------|
|1st   |`r firstClassPercent` |
|2st   |`r secondClassPercent`|
|3st   |`r thirdClassPercent` |

But helps to clarify the much larger casualties for 3rd-class. The myth is that the wealthy would forcibly keep non-wealthy from reaching the life-boats, but it seems like the ship was engineered to give the 3rd-class an disadvantage in the first place, even if not purposelessly in case of a disaster:

> "Gates did exist which barred the third class passengers from the other passengers. But this was not in anticipation of a shipwreck but in compliance with US immigration laws and the feared spread of infectious diseases.
Third class passengers included Armenians, Chinese, Dutch, Italians, Russians, Scandinavians and Syrians as well as those from the British Isles - all in search of a new life in America.
"Under American immigration legislation, immigrants had to be kept separate so that before the Titanic docked in Manhattan, it first stopped at Ellis Island - where the immigrants were taken for health checks and immigration processing," Howells says."

(From [the same BBC article](http://www.bbc.com/news/magazine-17515305) mentioned earlier)

```{r}
survivors <- nrow(train[train$Survived == 1, ])
firstClassSurvivorsPercent <- round(nrow(train[train$Pclass == 1 & train$Survived == 1, ]) * 100 / survivors)
secondClassSurvivorsPercent <- round(nrow(train[train$Pclass == 2 & train$Survived == 1, ]) * 100 / survivors)
thirdClassSurvivorsPercent <- round(nrow(train[train$Pclass == 3 & train$Survived == 1, ]) * 100 / survivors)
```

|Pclass|Percentage from Survivors      |
|------|-------------------------------|
|1st   |`r firstClassSurvivorsPercent` |
|2st   |`r secondClassSurvivorsPercent`|
|3st   |`r thirdClassSurvivorsPercent` |

Still, more people from 1st-class survived than any other class. Let's now add `Deck` to the records:

```{r}
extractDeck <- function(cabin) {
  return(toString(unique(strsplit(cabin, "[^A-Z]+")[[1]])))
}

addDeck <- function(data) {
  data$Deck <- sapply(data$Cabin, extractDeck)
  data[, "Deck"] <- as.factor(data[, "Deck"])
  return(data)
}

all <- addDeck(all)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Here's the count distribution per `Deck`:


```{r, fig.width = 12, fig.height = 12}
countBarchart(train, "Deck", "Overall")
```

I think that the data is too sparse to give us any insight, but here follows the survivorship for `Deck`:

```{r, fig.width = 12, fig.height = 12}
categoricalResultCountBarchart(train, "Deck", "Survived")
```

Doesn't quite support the claims in the article, but I believe that this is due to the sparsity of the data.

I'm going to try to fix this issue by using `Ticket` (people traveling together have the same ticket code). It's reasonable to assume that people traveling together might share cabins.
Maybe only one passenger from a sharing `Ticket` gets `Cabin`? Let's check:

```{r}
normalizeList <- function(elems) {
  elems <- unique(elems)
  elems <- elems[!is.na(elems)]
  s <- paste(elems, collapse = " ")
  if(str_length(trimws(s)) == 0) {
    s <- NA
  }
  return(s)
}

cabins <- all[with(all, order(Ticket)), which(names(all) %in% c("Ticket", "Cabin"))]
cabins <- cabins %>%
  group_by(Ticket) %>%
  summarise(Cabin = list(Cabin))
cabins$Cabin <- sapply(cabins$Cabin, normalizeList)

ticketsMissngCabinRows <- nrow(cabins[is.na(cabins$Cabin), ])
```

There are `r ticketsMissngCabinRows` missing `Cabin`, so it seems like that's not the case... `Cabin` seems just too sparse to use...

## `Ticket` Analysis

From fiddling around with data, I found out that people that embarked together share the same ticket code. I have grouped the data by `Ticket` and concatenated `Name`, `Surname`, `Title` and `Sex`. I've filtered the columns that seemed relevant and aggregated them by `Ticket`:

```{r}
aggregateFunction <- function(s) {
  return(paste(s, collapse = " ~ "))
}

tickets <- train[with(train, order(Ticket)), which(names(train) %in% c("Ticket", "Name", "Surname", "Title", "Sex"))]
tickets <- tickets %>%
  group_by(Ticket) %>%
  summarise(Names = aggregateFunction(Name), 
            Surnames = aggregateFunction(Surname), 
            Titles = aggregateFunction(Title), 
            Genders = aggregateFunction(Sex),
            People = n())
tickets <- tickets[tickets$People > 1, ]
```

I have also filtered out people traveling alone, as I'm interested in groups traveling together:

```{r, results="asis"}
renderTable(tickets)
```

<br/>
If you go through this table, you'll find whole families under the same `Ticket`. I guess that we could use `Ticket` to establish family connections among passengers, but `Surname` seems more reliable to me.

## Travel Buddies Ties

### Using `Ticket` to Compute Travel Buddies Survival Rate

From the previous section, we've found out that `Ticket` can be used to connect people traveling together. The question is, do "travel buddies" stick together? (in a similar way as families did?)

```{r}
all <- addSurvivalRate("Ticket", all, train)
train <- all[all$PassengerId %in% train$PassengerId, ]
test <- all[all$PassengerId %in% test$PassengerId, ]
```

Here's the distribution for `SurvivalRateByTicket`:

```{r, fig.width = 12, fig.height = 12}
countBarchart(train, "SurvivalRateByTicket", "Overall")
```

It seems like buddies do stick together, just like families. Of course most people traveling together are related, so `Ticket` could  probably be used as a proxy for `Surname'.

### Conclusion

<br/>
A significant fraction of travel buddies seem to survive or die together for the available data.

![Love you, man! Love you too, bud!](https://s5.postimg.org/ozi8p8p6v/i_love_you_man.png)

# Classifier

## Tunning the Classifier

Let's try some cross validation to find the best parameters for our random forest classifier. The parameters that can be tuned in a random forest classifier are the following:

|Parameter|Description                                                      |Comment                                                             |
|---------|-----------------------------------------------------------------|--------------------------------------------------------------------|
|ntree    |Number of trees.                                                 |Should be big enough to avoid over fitting (default is 500).         |
|mtry     |Number of variables randomly sampled as candidates at each split.|We are going to use cross validation to determine it (default is 5).|

Be warned that cross validation might take a few minutes to finish.

### Random Search Cross Validation

The following code does a random grid search:

```{r}
randomForestRandomCrossValidation <- function(fml, data, metric, radius, repeats) {
  set.seed(345)
  control <- trainControl(method="repeatedcv", number=10, repeats=repeats, search="random")
  
  rfCV <- train(fml, data=data, method="rf", metric=metric, tuneLength=radius, trControl=control, importance = TRUE)
  return(rfCV)
}

fml <-Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize
plot(randomForestRandomCrossValidation(fml, train, "Accuracy", 10, 1))
```

Be warned that this takes a while to complete (gets slower as the number of features and levels per factor increases):

```{r}
fml <-Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO + FamilyIDWO
#plot(randomForestRandomCrossValidation(fml, train, "Accuracy", 10, 3))
```

![Kaggle times out after 20 minutes, so commenting out this and posting the output as an image.](https://s5.postimg.org/mdwmv5vgn/random_forest_cv_plot.png)

Another alternative to find the optimal mtry is`tuneRF()` from randomForest, which uses [OOBError](https://en.wikipedia.org/wiki/Out-of-bag_error) but unfortunately cannot handle factors with more than 53 levels, so we can't include `FamilyID`, `FamilyIDWO`, `Surname` or `Ticket`:

```{r}
mtryTunning <- function(data) {
  set.seed(345)
  suppressMessages(mtry <- tuneRF(data[, names(data) %in% attr(terms(fml), "term.labels")],
                                  data[, names(data) %in% c("Survived")],
                                  stepFactor=3.0,
                                  improve=1e-8,
                                  ntree=500,
                                  trace=FALSE))
  return(mtry)
}

fml <-Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO
print(mtryTunning(train))
```

## Creating Models

From the cross validation we now know that the optimal value for mtry is 1. The next step is creating the classifier and evaluating them.

Being a coder, I always try to write decoupled code that can be re-used. R doesn't quite support [OOP](https://en.wikipedia.org/wiki/Object-oriented_programming), but you at least can come close in terms of emulating encapsulation. Given that functions are objects and you can define objects (including functions) inside objects, I have defined a class of objects named `Builder()`. They all share the same interface (functions `mode()`, `predictions()` and `probabilities()`). I did this so I would not have to duplicate much code. I created a function named `evaluatedModels()`, which provided a list of builders and formulas, can evaluate all of them (because the builders all obey the same contract):

```{r}
RandomForestBuilder <- function() {
  mtry = 1
  
  name = paste("Random Forest (randomForest) mtry:", mtry)
  
  model = function(fml, data) {
    set.seed(345)
    numVars <- length(attr(terms(fml), "term.labels"))
    maxMtry <- floor(sqrt(numVars))
    
    if (mtry > maxMtry) {
      return(randomForest(formula(fml), data = data))
    }
    return(randomForest(formula(fml), data = data, mtry = mtry))
  }
  
  predictions = function(model, data) {
    return(stats::predict(model, newdata = data, type = "class"))
  }
  
  probabilities = function(model, data) {
    result <- predict(model, newdata=data, type="prob")
    return(result[, 2])
  }
  
  return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
}

RandomForestRandomCVBuilder <- function() {
  name = "Random Forest with Random Search Cross Validation (rpart)"
  
  model = function(fml, data) {
    set.seed(345)
    return(randomForestRandomCrossValidation(fml, data, "Accuracy", radius=10, repeats=3))
  }
  
  predictions = function(model, data) {
    return(stats::predict(model, newdata = data, type = "raw"))
  }
  
  probabilities = function(model, data) {
    result <- predict(model, newdata=data, type="prob")
    return(result[, 2])
  }
  
  return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
}

DecisionTreeBuilder <- function() {
  name = "Decision Tree (rpart)"
  
  model = function(fml, data) {
    set.seed(345)
    return(rpart(fml, data = data, method="class"))
  }
  
  predictions = function(model, data) {
    return(stats::predict(model, newdata = data, type = "class"))
  }
  
  probabilities = function(model, data) {
    result <- predict(model, newdata=data, type="prob")
    return(result[, 2])
  }
  
  return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
}

ConditionalInferenceBuilder <- function() {
  mtry = 3
  
  name = paste("Conditional Inference Forest (party) mtry: ", mtry)
  
  model = function(fml, data) {
    set.seed(345)
    
    numVars <- length(attr(terms(fml), "term.labels"))
    maxMtry <- floor(sqrt(numVars))
    
    if (mtry > maxMtry) {
      return(cforest(fml, data = data, controls=cforest_unbiased()))
    }
    return(cforest(fml, data = data, controls=cforest_unbiased(mtry = mtry)))
  }
  
  predictions = function(model, data) {
    return(stats::predict(model, newdata = data, type = "response"))
  }
  
  probabilities = function(model, data) {
    result <- predict(model, newdata=data, type="prob")
    result <- lapply(result, `[`, 2)
    return(result)
  }
  
  return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
}

SVMBuilder <- function(fml, data) {
  name = "SVM (e1071)"
  
  model = function(fml, data) {
    set.seed(345)
    return(svm(formula(fml), data = data, probability=TRUE))
  }
  
  predictions = function(model, data) {
    return(stats::predict(model, newdata = data, type = "class"))
  }
  
  probabilities = function(model, data) {
    result <- stats::predict(model, newdata = data, probability=TRUE)
    return(attr(result, "probabilities")[, 2])
  }
  
  return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
}
```

You may plot the decision tree from `DecisionTreeBuilder` by using the "rattle" package:

```{r}
builder <- DecisionTreeBuilder()
fml <-Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO + FamilyIDWO
model <- builder$model(fml, train)
fancyRpartPlot(model)
```

You can actually see what's going on in the classification. Neat, Huhn? Simpler models have this advantage. For more sophisticated models like random forests that would not be so straightforward (even though you could sample a tree from a random forest for instance).

## Evaluating the Classifier

Kaggle doesn't provide labeled testing data, so I'm going to break the training data-set into two data-sets (training and testing) for evaluation purposes.

I have created functions for the purpose of evaluating performance for the models as well:

```{r}
fmeasure <- function(confusion) {
  p <- confusion$byClass["Pos Pred Value"]
  r <- confusion$byClass["Sensitivity"]
  f <- 2 * ((p * r) / (p + r))
  return(f)
}

accuracy <- function(confusion) {
  return(confusion$overall["Accuracy"])
}

createFormulaName <- function(fml) {
  formulaName <- paste(format(fml), collapse = "")
  formulaName <- str_replace_all(formulaName, "[\\s]", "")
  return(formulaName)
}

evaluateModel <- function(fml, ModelBuilder, predictionColumn, trainData, testData) {
  formula <- createFormulaName(fml)
  modelBuilder <- ModelBuilder()
  model <- modelBuilder$model(fml, trainData)
  predictions <- modelBuilder$predictions(model, testData)
  confusion <- confusionMatrix(data = predictions, reference = testData$Survived)
  evaluation <- data.frame(model = modelBuilder$name, 
                           formula = formula,
                           accuracy = accuracy(confusion), 
                           fmeasure = fmeasure(confusion), 
                           stringsAsFactors=FALSE)
  return(evaluation)
}

evaluateModels <- function(formulas, ModelBuilders, predictionColumn, data, testData = NULL) {
  set.seed(3456)
  if(is.null(testData)) {
    trainIdx <- createDataPartition(data$Survived, p = 0.6, list = FALSE, times = 1)
    trainData <- data[trainIdx,]
    testData <- data[-trainIdx,]
  } else {
    trainData <- data
  }
  
  evaluations <- data.frame(model = character(), 
                            formula = character(), 
                            accuracy = numeric(0), 
                            fmeasure = numeric(0), 
                            stringsAsFactors=FALSE)

  for (ModelBuilder in ModelBuilders) {
    for (fml in formulas) {
      evaluation <- evaluateModel(fml, ModelBuilder, predictionColumn, trainData, testData)
      evaluations <- bind_rows(evaluations, evaluation)
    }
  }
  renderTable(evaluations)
}
```

Note that I'm using [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) and [F-Measure](https://en.wikipedia.org/wiki/F1_score). I've seen F-Measure being used more often in machine learning (both in courses and at work), but I often compute both (accuracy seems more intuitive and easier for business to understand).

The [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) returned by `caret::confusionMatrix()` has many different metrics, for which you can find more about [here](https://topepo.github.io/caret/measuring-performance.html). The metrics don't include a F-Measure though, so it needs to be computed from existent ones. For more information on the available metrics refer to [this](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values) and [this](https://en.wikipedia.org/wiki/Precision_and_recall).

### Evaluation Aftermath (Random Forest & SVM) <a name="aftermath"></a>

Let's create some models using random forest and different groups of features:

```{r, results="asis"}
trainIdx <- createDataPartition(train$Survived, p = 0.6, list = FALSE, times = 1)
trainData <- train[trainIdx,]
testData <- train[-trainIdx,]
trainData <- addSurvivalRate("Surname", trainData, trainData)
testData <- addSurvivalRate("Surname", testData, trainData)


trainData <- addSurvivalRate("Ticket", trainData, trainData)
testData <- addSurvivalRate("Ticket", testData, trainData)

trainData <- addSurvivalRate("FamilyID", trainData, trainData)
testData <- addSurvivalRate("FamilyID", testData, trainData)

trainData <- addSurvivalRate("FamilyIDWO", trainData, trainData)
testData <- addSurvivalRate("FamilyIDWO", testData, trainData)

formulas <- c(Survived ~ Sex,
              Survived ~ Sex + Age,
              Survived ~ Sex + Age + Fare,
              Survived ~ Sex + Age + Fare + Pclass,
              Survived ~ Sex + Age + Fare + Pclass + SibSp,
              Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + Title,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + IsChild,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + IsChild + IsMother)

models <- c(RandomForestBuilder, SVMBuilder)

evaluateModels(formulas, models, "Survived", trainData, testData)
```

The following models are a bit slower to generate the classifier (due to cross validation), so I'm running them separately:

```{r, results="asis"}
formulas <- c(Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + Title,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO)

models <- c(DecisionTreeBuilder, ConditionalInferenceBuilder, RandomForestRandomCVBuilder)

#evaluateModels(formulas, models, "Survived", trainData, testData)
```

Kaggle times out after 20 minutes, so commenting out this and posting the output:

<!-- html table generated in R 3.3.2 by xtable 1.8-2 package -->
<!-- Wed Jan 18 01:59:44 2017 -->
<table border=1>
<tr> <th>  </th> <th> model </th> <th> formula </th> <th> accuracy </th> <th> fmeasure </th>  </tr>
  <tr> <td align="right"> 1 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 2 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+Title </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 3 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 4 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+AgeWO+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 5 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+AgeWO+FareWO+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.82 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 6 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked </td> <td align="right"> 0.81 </td> <td align="right"> 0.86 </td> </tr>
  <tr> <td align="right"> 7 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+Title </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 8 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 9 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+AgeWO+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.88 </td> </tr>
  <tr> <td align="right"> 10 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+AgeWO+FareWO+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 11 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 12 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+Title </td> <td align="right"> 0.85 </td> <td align="right"> 0.88 </td> </tr>
  <tr> <td align="right"> 13 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.84 </td> <td align="right"> 0.88 </td> </tr>
  <tr> <td align="right"> 14 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+AgeWO+Fare+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.83 </td> <td align="right"> 0.87 </td> </tr>
  <tr> <td align="right"> 15 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+AgeWO+FareWO+Pclass+FamilySize+Embarked+TitleWO </td> <td align="right"> 0.84 </td> <td align="right"> 0.88 </td> </tr>
   </table>



<br/>
I'm also only running them for the most promising models.


As a final touch, I'm going to plot the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves.

```{r, fig.width = 16, fig.height = 8}
createPrediction <- function(fml, ModelBuilder, testData) {
  modelBuilder <- ModelBuilder()
  model <- modelBuilder$model(fml, trainData)
  probabilities <- modelBuilder$probabilities(model, testData)
  return(prediction(as.numeric(probabilities), testData$Survived))
}

createPerformance <- function(prediction) {
  return(performance(prediction, measure = "tpr", x.measure = "fpr"))
}

createAuc <- function(prediction) {
  auc <- performance(prediction, measure = "auc")
  return(auc@y.values[[1]])
}

buildModelName <- function(fml, ModelBuilder, auc) {
  formulaName <- createFormulaName(fml)
  aucName <- paste("(AUC: ", auc, ")")
  modelBuilder <- ModelBuilder()
  name <- paste0(modelBuilder$name, "\n", formulaName, "\n", aucName, "\n")
  return(name)
}

createRocPlot <- function(roc) {
  return(list(geom_ribbon(data = roc, aes(x=FPR, fill = Model, ymin=0, ymax=TPR), alpha = 0.2),
              geom_line(data = roc, aes(x=FPR, y=TPR, color = Model))))
}

generateAllROCs <- function(formulas, ModelBuilders, trainData, testData) {
  rocPlots <- c()
  for (ModelBuilder in ModelBuilders) {
    for (fml in formulas) {
      modelBuilder <- ModelBuilder()
      prediction <- createPrediction(fml, ModelBuilder, testData)
      performance <- createPerformance(prediction)
      auc <- createAuc(prediction)
      roc <- data.frame(FPR=unlist(performance@x.values),
                        TPR=unlist(performance@y.values),
                        Model=rep(buildModelName(fml, ModelBuilder, auc), each=length(performance@x.values)))
      rocPlots <- c(rocPlots, createRocPlot(roc))
    }
  }
  
  roc <- data.frame(FPR=c(0.0, 1.0),
                    TPR=c(0.0, 1.0),
                    Model=rep("Line of No-discrimination\n", each=2))
  rocPlots <- c(rocPlots, createRocPlot(roc))
  ggplot() + rocPlots + coord_fixed()
}

formulas <- c(Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + Fare + Pclass + FamilySize + Embarked + TitleWO)

models <- c(RandomForestBuilder, SVMBuilder, DecisionTreeBuilder)

generateAllROCs(formulas, models, trainData, testData)
```

These two take a while to complete, so I'm running them separatedly:

```{r, fig.width = 16, fig.height = 8}
models <- c(ConditionalInferenceBuilder, RandomForestRandomCVBuilder)

#generateAllROCs(formulas, models, trainData, testData)
```

![Kaggle times out after 20 minutes, so commenting out this and posting the output as an image.](https://s5.postimg.org/wt2t42snb/roc_plot1.png)

### Evaluation Aftermath II: This Time is Personal (Decision Tree, Conditional Inference Forest & Cross Validation Random Forest) <a name="aftermath2"></a>

We can't use `Surname` or `Ticket` with randomForest package (too many categories for `Surname`), so I'm going to run them separately:


```{r, results="asis"}
formulas <- c(Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO + FamilyIDWO)

models <- c(SVMBuilder, DecisionTreeBuilder, ConditionalInferenceBuilder, RandomForestRandomCVBuilder)
#evaluateModels(formulas, models, "Survived", train)
```

Kaggle times out after 20 minutes, so commenting out this and posting the output:

<!-- html table generated in R 3.3.2 by xtable 1.8-2 package -->
<!-- Wed Jan 18 12:38:16 2017 -->
<table border=1>
<tr> <th>  </th> <th> model </th> <th> formula </th> <th> accuracy </th> <th> fmeasure </th>  </tr>
  <tr> <td align="right"> 1 </td> <td> SVM (e1071) </td> <td> Survived~Sex+Age+Fare+Pclass+SibSp+Parch+FamilySize+Embarked+TitleWO+FamilyIDWO </td> <td align="right"> 0.80 </td> <td align="right"> 0.84 </td> </tr>
  <tr> <td align="right"> 2 </td> <td> Decision Tree (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+SibSp+Parch+FamilySize+Embarked+TitleWO+FamilyIDWO </td> <td align="right"> 0.79 </td> <td align="right"> 0.83 </td> </tr>
  <tr> <td align="right"> 3 </td> <td> Conditional Inference Forest (party) mtry:  3 </td> <td> Survived~Sex+Age+Fare+Pclass+SibSp+Parch+FamilySize+Embarked+TitleWO+FamilyIDWO </td> <td align="right"> 0.82 </td> <td align="right"> 0.86 </td> </tr>
  <tr> <td align="right"> 4 </td> <td> Random Forest with Random Search Cross Validation (rpart) </td> <td> Survived~Sex+Age+Fare+Pclass+SibSp+Parch+FamilySize+Embarked+TitleWO+FamilyIDWO </td> <td align="right"> 0.79 </td> <td align="right"> 0.83 </td> </tr>
   </table>

<br/>
Here are the correspondent ROC curves:

```{r, fig.width = 16, fig.height = 8}
models <- c(DecisionTreeBuilder, ConditionalInferenceBuilder, RandomForestRandomCVBuilder)

#generateAllROCs(formulas, models, trainData, testData)
```

![Kaggle times out after 20 minutes, so commenting out this and posting the output as an image.](https://s5.postimg.org/upsdwesuf/roc_plot2.png)

# Feature Importance

I believe we've exhausted all features we could extract, so we can now build a final model and evaluate their variables' importance:

```{r, fig.width = 12, fig.height = 6}
customtImpPlot <- function(model, title) {
  if(!is(model, "RandomForest") & !is(model, "randomForest")) {
    model <- model$finalModel
  }
  if (is(model, "randomForest")) {
    varImpPlot(model, type=2, main=title)
  } else {
    importance <- varimp(model)
    dotchart(importance[order(importance)], main = title)
  }
}

varImpPlotAll <- function(formulas, ModelBuilders) {
    par(mfrow=c(length(formulas), length(ModelBuilders))) 
    varImpPlots <- c()
    for (ModelBuilder in ModelBuilders) {
      for (fml in formulas) {
        builder <- ModelBuilder()
        model <- builder$model(formula(fml), data=train)
        title <- paste(builder$name, "\n", createFormulaName(fml))
        customtImpPlot(model, title)
      }
    }
}

formulas <- c(Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO)

models <- c(RandomForestBuilder)

varImpPlotAll(formulas, models)
```

`randomForest` doesn't support more than 53 categories (factor levels), so we can build a model with `FamilyIDWO`.

```{r, fig.width = 12, fig.height = 12}
formulas <- c(Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + FamilyIDWO)

models <- c(ConditionalInferenceBuilder)

#varImpPlotAll(formulas, models)
```

![Kaggle times out after 20 minutes, so commenting out this and posting the output as an image.](https://s5.postimg.org/gkmkuljt3/var_imp_plot.png)

# Generating Predictions

We need also to apply all the transformations we came up with during feature engineering:

Now we can create a classifier and generate the predictions:

```{r}
buildOutputName <- function(fml, modelName) {
  formulaName <- paste0(format(fml), collapse = "")
  output <- paste0(modelName, formulaName)
  output <- str_replace_all(output, "[^\\w]", "")
  output <- paste0(output, ".csv")
  return(output)
}

generatePredictions <- function(fml, ModelBuilder, predictionColumn, trainData, testData) {
  modelBuilder <- ModelBuilder()
  model <- modelBuilder$model(fml, trainData)
  predictions <- modelBuilder$predictions(model, testData)
  solution <- data.frame(PassengerID = testData$PassengerId, Survived = predictions)
  output <- buildOutputName(fml, modelBuilder$name)
  write.csv(solution, file = output, row.names = FALSE)
  testData$Survived <- predictions
  return(testData)
}

generateAllPredictions <- function(formulas, ModelBuilders, predictionColumn, trainData, testData) {
  for (ModelBuilder in ModelBuilders) {
    for (fml in formulas) {
      evaluation <- generatePredictions(fml, ModelBuilder, predictionColumn, trainData, testData)
    }
  }
}

fml <-Survived ~ Sex + AgeWO + FareWO + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO + FamilyIDWO
test <- generatePredictions(fml, ConditionalInferenceBuilder, "Survived", train, test)
```

# Sanity Check

I've saved the predictions to `test$Survived`, this way we can quick check it through plotting.

```{r}
countBarchart(test, "Survived", "Overall")
```

I've submitted a trivial model to Kaggle where everybody survived to measure the percentage of survivors. There are about 63% survivors in the testing data-set, which is about the same percentage of survivors in the training data-set. That's a nice sanity check: If your predictions are any good, the percentage of survivors in your predictions should be close to this value.

```{r}
categoricalResultCountBarchart(test, "Sex", "Survived")
```

Female do better than male.

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(test, "AgeWO", "Survived", 10)
```

Children do better than adults.

```{r, fig.width = 12, fig.height = 12}
categoricalResultHistogram(test, "FareWO", "Survived", 10)
```

```{r}
categoricalResultCountBarchart(test, "Pclass", "Survived")
```

Wealthy do better than poor. The predictions seem consistent with the observations.

# Kaggle Submission

My latest submission achieved 0.82775 accuracy, which seems like a very good result (consult the Kaggle discussion fora). I scrutinized this data as much as I could (haven't found a more comprehensive analysis anywhere). 

> **NOTE:**
> If you're a beginner: Don't get discouraged by the top scores. Unfortunately, it's pretty easy to cheat on this challenge (given that you can find the list of survivors online), thus you will find 100% accuracy scores... I would say that these are definitely cheats, while anything much greater than 80% are probably cheats as well.